{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "##### In this project, I explored the fascinating intersection of machine learning and film. <br> I built machine learning models and trained them on a collection of movies to assess how likely I am to re-watch some of my favorite films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Load and Setup\n",
    "##### Initialize Missing Values For the Following Columns: Re-Watch Desire, Wins, Losses, and Ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import numpy as np\n",
    "from imdb import IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Initialize Data Function\n",
    "def Initialize_Data(filename):\n",
    "    Data = pd.read_csv(filename)\n",
    "    Data['Re-Watch Desire'] = 5.0\n",
    "    Data['Wins'] = 0\n",
    "    Data['Losses'] = 0\n",
    "    Data['Ties'] = 0\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Initial Set of Movies\n",
    "Movies = Initialize_Data('Data/Movies List.csv')\n",
    "Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Movie Comparison and Feature Engineering\n",
    "##### Utilizing pairwise comparison and feature engineering, I created preference data and fields encompassing the following themes: <br> Genre, Runtime, Budget, and More!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pairwise Movie Comparison Function\n",
    "def compare_movies(Movies, Start, End, Comparison_Results):\n",
    "\n",
    "    Titles = list(Movies['Movie'][Start:End])\n",
    "                  \n",
    "    #  Generate All Pairs for Comparison\n",
    "    for i, movie1 in enumerate(Titles[:-1]):\n",
    "        for movie2 in Titles[i+1:]:\n",
    "            \n",
    "            Valid_Input = False\n",
    "            while not Valid_Input:\n",
    "            \n",
    "                print(f'Comparison: {movie1} vs {movie2}')\n",
    "                Choice = input('Which movie do you prefer? Enter 1 for the first, 2 for the second, or 3 for a tie: ')\n",
    "\n",
    "                if Choice in ['1', '2', '3']:\n",
    "                    Valid_Input = True\n",
    "                    \n",
    "                    if Choice == '1':\n",
    "                        Movies.loc[Movies['Movie'] == movie1, 'Re-Watch Desire'] += .1\n",
    "                        Movies.loc[Movies['Movie'] == movie2, 'Re-Watch Desire'] -= .1\n",
    "                        Movies.loc[Movies['Movie'] == movie1, 'Wins'] += 1\n",
    "                        Movies.loc[Movies['Movie'] == movie2, 'Losses'] += 1\n",
    "                        Comparison_Result = f'{movie1} beat {movie2}'\n",
    "                        print(f'Result: {movie1} wins this round!')\n",
    "                \n",
    "                    elif Choice == '2':\n",
    "                        Movies.loc[Movies['Movie'] == movie1, 'Re-Watch Desire'] -= .1\n",
    "                        Movies.loc[Movies['Movie'] == movie2, 'Re-Watch Desire'] += .1\n",
    "                        Movies.loc[Movies['Movie'] == movie1, 'Losses'] += 1\n",
    "                        Movies.loc[Movies['Movie'] == movie2, 'Wins'] += 1\n",
    "                        Comparison_Result = f'{movie2} beat {movie1}'\n",
    "                        print(f'Result: {movie2} wins this round!')\n",
    "                    \n",
    "                    elif Choice == '3':\n",
    "                        Movies.loc[Movies['Movie'] == movie1, 'Ties'] += 1\n",
    "                        Movies.loc[Movies['Movie'] == movie2, 'Ties'] += 1\n",
    "                        Comparison_Result = f'{movie1} tied {movie2}'\n",
    "                        print(f'Result: tie between {movie1} and {movie2}')\n",
    "                    \n",
    "                    Movies['Re-Watch Desire'] = Movies['Re-Watch Desire'].clip(0, 10)\n",
    "                    Comparison_Results.append(Comparison_Result)\n",
    "\n",
    "                else:\n",
    "                    print('Invalid input. Please enter 1, 2, or 3.')\n",
    "        \n",
    "    return Movies, Comparison_Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Pairwise Function and the Evaluate First Ten Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the First Ten Movies\n",
    "Movies_Df, Comparison_Log = compare_movies(Movies, 0, 10, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Progress Made to Movies Dataset and Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(Movies, Comparison_Results, Movie_Filename='Data/movie_comparisons.csv', Results_Filename='Data/comparison_results.csv'):\n",
    "    Movies.to_csv(Movie_Filename, index=False)\n",
    "    Results_Df = pd.DataFrame(Comparison_Results, columns=['Comparison'])\n",
    "    Results_Df.to_csv(Results_Filename, index=False)\n",
    "    print(f'Progress saved to {Movie_Filename} and {Results_Filename}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_progress(Movies_Df, Comparison_Log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Progress Made to Movies Dataset and Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress(Movie_Filename = 'Data/movie_comparisons.csv', Results_Filename = 'Data/comparison_results.csv'):\n",
    "    try:\n",
    "        Movies = pd.read_csv(Movie_Filename)\n",
    "        Results_Df = pd.read_csv(Results_Filename)\n",
    "        Comparison_Results = Results_Df['Comparison'].tolist()\n",
    "        return Movies, Comparison_Results\n",
    "    except:\n",
    "        print(f'No Save file found. Starting Fresh.')\n",
    "        return None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Movie_Comparisons and Comparison Results\n",
    "load_progress();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define How Movies will be Evaluated in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_movies_in_batches(Movies, Batch_Size, Comparison_Results):\n",
    "    # Separate Evaluated and Non-Evaluated Movies Based on Whether They Have Any Wins, Losses or Ties\n",
    "    Evaluated_Movies = Movies[(Movies['Wins'] > 0) | (Movies['Losses'] > 0) | (Movies['Ties'] > 0)]\n",
    "    Non_Evaluated_Movies = Movies[(Movies['Wins'] == 0) & (Movies['Losses'] == 0) & (Movies['Ties'] == 0)]\n",
    "\n",
    "    # Process Only One Batch of Non-Evaluated Movies\n",
    "    Batch = Non_Evaluated_Movies.head(Batch_Size)\n",
    "    if not Batch.empty:\n",
    "        Start = Batch.index[0]\n",
    "        End = Batch.index[-1] + 1\n",
    "        Movies, Comparison_Results = compare_movies(Movies, Start, End, Comparison_Results)\n",
    "\n",
    "        # Compare Evaluated Movies with Non-Evaluated Movies in the Current Batch\n",
    "        for _, Row in Evaluated_Movies.iterrows():\n",
    "            for _, Batch_Row in Batch.iterrows():\n",
    "                Valid_Input = False\n",
    "                while not Valid_Input:\n",
    "                    print(f'Comparison: {Row['Movie']} vs {Batch_Row['Movie']}')\n",
    "                    Choice = input('Which movie do you prefer? Enter 1 for the first, 2 for the second, or 3 for a tie: ')\n",
    "                    \n",
    "                    if Choice in ['1', '2', '3']:\n",
    "                        Valid_Input = True\n",
    "\n",
    "                        if Choice == '1':\n",
    "                            Movies.loc[Movies['Movie'] == Row['Movie'], 'Re-Watch Desire'] += .1\n",
    "                            Movies.loc[Movies['Movie'] == Batch_Row['Movie'], 'Re-Watch Desire'] -= .1\n",
    "                            Movies.loc[Movies['Movie'] == Row['Movie'], 'Wins'] += 1\n",
    "                            Movies.loc[Movies['Movie'] == Batch_Row['Movie'], 'Losses'] += 1\n",
    "                            Comparison_Result = f'{Row['Movie']} beat {Batch_Row['Movie']}'\n",
    "                            print(f'Result: {Row['Movie']} wins this round!')\n",
    "\n",
    "                        elif Choice == '2':\n",
    "                            Movies.loc[Movies['Movie'] == Row['Movie'], 'Re-Watch Desire'] -= .1\n",
    "                            Movies.loc[Movies['Movie'] == Batch_Row['Movie'], 'Re-Watch Desire'] += .1\n",
    "                            Movies.loc[Movies['Movie'] == Row['Movie'], 'Losses'] += 1\n",
    "                            Movies.loc[Movies['Movie'] == Batch_Row['Movie'], 'Wins'] += 1\n",
    "                            Comparison_Result = f'{Batch_Row['Movie']} beat {Row['Movie']}'\n",
    "                            print(f'Result: {Batch_Row['Movie']} wins this round!')\n",
    "\n",
    "                        elif Choice == '3':\n",
    "                            Movies.loc[Movies['Movie'] == Row['Movie'], 'Ties'] += 1\n",
    "                            Movies.loc[Movies['Movie'] == Batch_Row['Movie'], 'Ties'] += 1\n",
    "                            Comparison_Result = f'{Row['Movie']} tied {Batch_Row['Movie']}'\n",
    "                            print(f'Result: tie between {Row['Movie']} and {Batch_Row['Movie']}')\n",
    "\n",
    "                        Movies['Re-Watch Desire'] = Movies['Re-Watch Desire'].clip(0, 10)\n",
    "                        Comparison_Results.append(Comparison_Result)\n",
    "                    else:\n",
    "                        print('Invalid input. Please enter 1, 2, or 3.')\n",
    "\n",
    "    return Movies, Comparison_Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Movies in Batches, Evaluating Five Movies at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing with Batch Size of 5\n",
    "Batch_Size = 5\n",
    "Movies, Comparison_Results = load_progress('Data/movie_comparisons.csv', 'Data/comparison_results.csv')\n",
    "if Movies is None:\n",
    "   Movies = pd.DataFrame(Data)\n",
    "if not Comparison_Results:\n",
    "   Comparison_Results = []\n",
    "Movies, Comparison_Results = evaluate_movies_in_batches(Movies, Batch_Size=Batch_Size, Comparison_Results=Comparison_Results)\n",
    "save_progress(Movies, Comparison_Results, 'Data/movie_comparisons.csv', 'Data/comparison_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Movie Details from IMDb and Create New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_movie_details(input_file, output_file):\n",
    "    \n",
    "    # Create an Instance of the IMDb class\n",
    "    ia = IMDb()\n",
    "\n",
    "    # Load the CSV file with the Movie Comparisons\n",
    "    Movie_Comparisons = pd.read_csv(input_file)\n",
    "\n",
    "    # Extract the list of Movie Titles\n",
    "    Movies = Movie_Comparisons['Movie'].tolist()\n",
    "\n",
    "    # Initialize List to Store Movie Details\n",
    "    Details = []\n",
    "\n",
    "    # MPAA Rating to Numeric Scale Mapping\n",
    "    MPAA_Scale = {\n",
    "        'G': 1,\n",
    "        'PG': 2,\n",
    "        'PG-13': 3,\n",
    "        'R': 4,\n",
    "        'N/A': 0 # Absence of Rating\n",
    "    }\n",
    "\n",
    "    for Movie in Movies:\n",
    "        Search = ia.search_movie(Movie)\n",
    "        if Search:\n",
    "            Movie_Id = Search[0].movieID\n",
    "            Movie_Info = ia.get_movie(Movie_Id)\n",
    "\n",
    "            # Extract Details\n",
    "            Title = Movie_Info.get('title', 'N/A')\n",
    "            Rating = Movie_Info.get('rating', 'N/A')\n",
    "            Runtime = Movie_Info.get('runtime', ['N/A'])[0] # Runtime in Minutes\n",
    "            Box_Office = Movie_Info.get('box office', {}).get('Cumulative Worldwide Gross', 'N/A')\n",
    "            Genre = ', '.join(Movie_Info.get('genres', 'N/A'))\n",
    "\n",
    "            # Extract MPAA Rating (Certificates)\n",
    "            Certificates = Movie_Info.get('certificates', [])\n",
    "            MPAA_Rating = 'N/A'\n",
    "            MPAA_Numeric = 0 # Default to 0 if no rating is found\n",
    "            for Certificate in Certificates:\n",
    "                if 'United States' in Certificate:\n",
    "                    MPAA_Rating = Certificate.split(':')[-1]\n",
    "                    MPAA_Numeric = MPAA_Scale.get(MPAA_Rating, 0)\n",
    "                    break\n",
    "\n",
    "            # Extract Budget and Convert to Millions\n",
    "            Budget_Str = Movie_Info.get('box office', {}).get('Budget', 'N/A')\n",
    "            Budget_Millions = 'N/A'\n",
    "            if Budget_Str != 'N/A' and Budget_Str.startswith('$'):\n",
    "                Budget_Value = Budget_Str.replace('$', '').replace(' ', '').replace(',', '').split('(')[0]\n",
    "                try:\n",
    "                    if 'million' in Budget_Value.lower():\n",
    "                        Budget_Millions = float(Budget_Value.lower().replace('million', '').strip())\n",
    "                    elif 'thousand' in Budget_Value.lower():\n",
    "                        Budget_Millions = float(Budget_Value.lower().replace('thousand', '').strip()) / 1000\n",
    "                    else:\n",
    "                        Budget_Millions = float(Budget_Value) / 1e6\n",
    "                except ValueError:\n",
    "                    Budget_Millions = 'N/A'\n",
    "\n",
    "            # Determine if the Movie Won Any Awards\n",
    "            Awards = Movie_Info.get('awards', {})\n",
    "            Award_Winner = 1 if Awards else 0\n",
    "\n",
    "            # Determine if the Movie was Filmed Outside of the United States and Canada\n",
    "            Locations = Movie_Info.get('filming locations', [])\n",
    "            Foreign_Film = 0\n",
    "            for Location in Locations:\n",
    "                if 'USA' not in Location and 'Canada' not in Location:\n",
    "                    Foreign_Film = 1\n",
    "                    break\n",
    "\n",
    "            # Append the Movie Details to the List\n",
    "            Details.append([Title, Rating, Box_Office, MPAA_Numeric, Award_Winner, Foreign_Film, Runtime, \n",
    "                            Budget_Millions, Genre])\n",
    "\n",
    "    # Create a DataFrame with the Movie Details\n",
    "    Movie_Details = pd.DataFrame(Details, columns=['Movie', 'IMDb Rating', 'Box Office', 'MPAA Numeric', \n",
    "                                                   'Award Winner', 'Foreign Film', 'Runtime (min)', 'Budget (In Millions)', \n",
    "                                                   'Genre'])\n",
    "\n",
    "    # Split Genre into Separate Columns\n",
    "    Genres_Split = Movie_Details['Genre'].str.get_dummies(sep=', ')\n",
    "\n",
    "    # Combine Movie Details with the New Columns\n",
    "    Modified_Data = pd.concat([Movie_Details, Genres_Split], axis=1)\n",
    "\n",
    "    # Save the DataFrame to CSV file\n",
    "    Modified_Data.to_csv(output_file, index=False)\n",
    "    print(f'Movie details saved to {output_file}')\n",
    "\n",
    "# Call Function to Scrape Movie Details\n",
    "fetch_movie_details('Data/movie_comparisons.csv', 'Data/movie_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Data Cleaning was Done Afterwards to Ensure Movie Details are Accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Movie Details Data with Rankings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Movies Ranking Data and Merge it with Movie Details\n",
    "Movie_Details = pd.read_csv('Data/movie_details.csv')\n",
    "Movie_Comparisons = pd.read_csv('Data/movie_comparisons.csv')\n",
    "\n",
    "# Merging the Datasets on the Movie Field \n",
    "Merged_Data = pd.merge(Movie_Comparisons, Movie_Details, on='Movie', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "##### I prepared Random Forest and XGBoost models on the newly created preference data, evaluating their predictive accuracy. <br> I then evaluated these models to identify the best model to generate new predictions on additional movies, assessing the re-watch potential of each film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Initial Features\n",
    "Initial_Features = [\n",
    "    'IMDb Rating', 'Box Office (In Millions)', 'MPAA Numeric', 'Award Winner', 'Foreign Film', 'Runtime (min)', \n",
    "    'Budget (In Millions)', 'Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Drama', \n",
    "    'Family', 'Fantasy', 'History', 'Music', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Sport', \n",
    "    'Thriller', 'War'\n",
    "]\n",
    "\n",
    "X = Merged_Data[Initial_Features]\n",
    "Y = Merged_Data['Re-Watch Desire']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Correlation between Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to identify strongly correlated features\n",
    "def find_strong_correlations(Data, Threshold=0.7):\n",
    "    # Calculate the correlation matrix\n",
    "    Corr_Matrix = Data.corr()\n",
    "    \n",
    "    # Find pairs of features with correlation above the threshold\n",
    "    Strong_Correlations = []\n",
    "    for i in range(len(Corr_Matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(Corr_Matrix.iloc[i, j]) > Threshold:\n",
    "                Strong_Correlations.append((Corr_Matrix.columns[i], Corr_Matrix.columns[j], Corr_Matrix.iloc[i, j]))\n",
    "    \n",
    "    # Return the strongly correlated features as a DataFrame\n",
    "    return pd.DataFrame(Strong_Correlations, columns=['Feature 1', 'Feature 2', 'Correlation'])\n",
    "\n",
    "# Example usage of the function\n",
    "Strong_Correlations = find_strong_correlations(X, Threshold=0.7)\n",
    "\n",
    "# Display the strong correlations DataFrame\n",
    "print(Strong_Correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/Correlations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Features to Remove Strong Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Features (removed 'Adventure', 'Animation', and 'Family')\n",
    "\n",
    "''' \n",
    "Initial_Features = [\n",
    "    'IMDb Rating', 'Box Office (In Millions)', 'MPAA Numeric', 'Award Winner', 'Foreign Film', 'Runtime (min)', \n",
    "    'Budget (In Millions)', 'Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Drama', \n",
    "    'Family', 'Fantasy', 'History', 'Music', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Sport', \n",
    "    'Thriller', 'War'\n",
    "]\n",
    "'''\n",
    "\n",
    "# Select Relevant Features (23 Features in Total)\n",
    "Relevant_Features = [\n",
    "    'IMDb Rating', 'Box Office (In Millions)', 'MPAA Numeric', 'Award Winner', 'Foreign Film', 'Runtime (min)',\n",
    "    'Budget (In Millions)', 'Action', 'Biography', 'Comedy', 'Crime', 'Drama', 'Fantasy', 'History', 'Music',\n",
    "    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Sport', 'Thriller', 'War'\n",
    "]\n",
    "\n",
    "X_Updated = Merged_Data[Relevant_Features]\n",
    "Y = Merged_Data['Re-Watch Desire']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Random Forest Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the Data into Training and Testing sets\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X_Updated, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set up the hyperparameter grid for tuning the Random Forest model\n",
    "Param_Grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "RF_Model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to find the best parameters\n",
    "Grid_Search = GridSearchCV(estimator=RF_Model, param_grid=Param_Grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "Grid_Search.fit(X_Train, Y_Train)\n",
    "\n",
    "# Select the best estimator\n",
    "Best_RF_Model = Grid_Search.best_estimator_\n",
    "\n",
    "# Train the best model on the training data\n",
    "Best_RF_Model.fit(X_Train, Y_Train)\n",
    "\n",
    "# Make Predictions on the Test Set\n",
    "Y_Pred = Best_RF_Model.predict(X_Test)\n",
    "\n",
    "# Evaluate the Model\n",
    "MAE = mean_absolute_error(Y_Test, Y_Pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test, Y_Pred))\n",
    "\n",
    "print(f'Mean Absolute Error: {MAE}')\n",
    "print(f'Root Mean Squared Error: {RMSE}')\n",
    "\n",
    "# Evaluate if Overfitting Occurred (Training Data)\n",
    "Y_Train_Pred = Best_RF_Model.predict(X_Train)\n",
    "Train_MAE = mean_absolute_error(Y_Train, Y_Train_Pred)\n",
    "Train_RMSE = np.sqrt(mean_squared_error(Y_Train, Y_Train_Pred))\n",
    "\n",
    "print(f'Training MAE: {Train_MAE}')\n",
    "print(f'Training RMSE: {Train_RMSE}')\n",
    "\n",
    "# Cross-validation on training data to assess overfitting\n",
    "CV_Scores = cross_val_score(Best_RF_Model, X_Train, Y_Train, cv=5, scoring='neg_mean_absolute_error')\n",
    "Mean_CV_Score = -np.mean(CV_Scores)\n",
    "\n",
    "print(f'Cross-Validation MAE: {Mean_CV_Score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Random Forest:\n",
    "Mean Absolute Error (MAE): 1.9467821317003164\n",
    "Root Mean Squared Error (RMSE): 2.372680695943936\n",
    "Training MAE: 1.8986799415670812\n",
    "Training RMSE: 2.259468144727309\n",
    "Cross-Validation MAE: 2.3795859173805205\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save RF Model for Easy Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(Best_RF_Model, 'Models/best_rf_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare XGBoost Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Training and Testing sets\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X_Updated, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set up the hyperparameter grid for tuning the XGBoost model\n",
    "Param_Grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "XGB_Model = xgb.XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# Use GridSearchCV to find the best parameters\n",
    "Grid_Search = GridSearchCV(estimator=XGB_Model, param_grid=Param_Grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "Grid_Search.fit(X_Train, Y_Train)\n",
    "\n",
    "# Select the best estimator\n",
    "Best_XGB_Model = Grid_Search.best_estimator_\n",
    "\n",
    "# Train the best model on the training data\n",
    "Best_XGB_Model.fit(X_Train, Y_Train)\n",
    "\n",
    "# Make Predictions on the Test Set\n",
    "Y_Pred = Best_XGB_Model.predict(X_Test)\n",
    "\n",
    "# Evaluate the Model\n",
    "MAE = mean_absolute_error(Y_Test, Y_Pred)\n",
    "RMSE = np.sqrt(mean_squared_error(Y_Test, Y_Pred))\n",
    "\n",
    "print(f'Mean Absolute Error: {MAE}')\n",
    "print(f'Root Mean Squared Error: {RMSE}')\n",
    "\n",
    "# Evaluate if Overfitting Occurred (Training Data)\n",
    "Y_Train_Pred = Best_XGB_Model.predict(X_Train)\n",
    "Train_MAE = mean_absolute_error(Y_Train, Y_Train_Pred)\n",
    "Train_RMSE = np.sqrt(mean_squared_error(Y_Train, Y_Train_Pred))\n",
    "\n",
    "print(f'Training MAE: {Train_MAE}')\n",
    "print(f'Training RMSE: {Train_RMSE}')\n",
    "\n",
    "# Cross-validation on training data to assess overfitting\n",
    "CV_Scores = cross_val_score(Best_XGB_Model, X_Train, Y_Train, cv=5, scoring='neg_mean_absolute_error')\n",
    "Mean_CV_Score = -np.mean(CV_Scores)\n",
    "\n",
    "print(f'Cross-Validation MAE: {Mean_CV_Score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best Hyperparameters: {'colsample_bytree': 1.0, 'gamma': 5, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 1, 'reg_lambda': 2, 'subsample': 0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "XGBoost Results:\n",
    "Mean Absolute Error: 1.9968243885040287\n",
    "Root Mean Squared Error: 2.5532722662428746\n",
    "Training MAE: 1.7757338047027589\n",
    "Training RMSE: 2.107246647250733\n",
    "Cross-Validation MAE: 2.343489589180265\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save XGB Model for Easy Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(Grid_Search.best_estimator_, 'Models/best_xgb_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Random Forest:\n",
    "Mean Absolute Error (MAE): 1.9467821317003164\n",
    "Root Mean Squared Error (RMSE): 2.372680695943936\n",
    "Training MAE: 1.8986799415670812\n",
    "Training RMSE: 2.259468144727309\n",
    "Cross-Validation MAE: 2.3795859173805205\n",
    "\n",
    "XGBoost Results:\n",
    "Mean Absolute Error (MAE): 1.9968243885040287\n",
    "Root Mean Squared Error (RMSE): 2.5532722662428746\n",
    "Training MAE: 1.7757338047027589\n",
    "Training RMSE: 2.107246647250733\n",
    "Cross-Validation MAE: 2.343489589180265\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways From Model Comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAE & RMSE <br> Random Forest is performing better on the test set in terms of MAE and RMSE, suggesting it is better at generalizing to unseen data. <br> XGBoost has better training performance but might be slightly overfitting the data. <br><br> Cross-Validation MAE <br> XGBoost has a marginally better Cross-Validation MAE, indicating that it has better performance across different splits of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Given that Random Forest has lower test error (MAE & RMSE), Random Forest is the better model in this case for generalization on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Details from IMDb for New Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Movie Details: DiDi, Forrest Gump, American Psycho, Iron Man, American Beauty\n",
    "fetch_movie_details('Data/new_movies.csv', 'Data/new_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Data Cleaning was Done Afterwards to Ensure Movie Details are Accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the Random Forest Model to New Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Random Forest model from the saved file\n",
    "Best_RF_Model = joblib.load('Models/best_rf_model.pkl')\n",
    "\n",
    "# Load New Movie Details\n",
    "New_Details = pd.read_csv('Data/new_details.csv')\n",
    "\n",
    "# Use the model to predict the 'Re-Watch Desire' for the new movies\n",
    "Rewatch_Predictions = Best_RF_Model.predict(New_Details.drop(columns=['Movie']))\n",
    "\n",
    "# Create a new dataframe with only 'Movie' and 'Predicted Re-Watch Desire'\n",
    "Results = New_Details[['Movie']].copy()\n",
    "Results['Predicted Re-Watch Desire'] = Rewatch_Predictions\n",
    "\n",
    "# Print the results\n",
    "print(Results)\n",
    "\n",
    "# Save the results to 'RF_Predictions.csv'\n",
    "Results.to_csv('Data/RF_Predictions.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/RF_Predictions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Pairwise Comparison on New Movies to Figure out Actual Re-Watch Desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing with Batch Size of 5\n",
    "Batch_Size = 5\n",
    "Movies, Comparison_Results = load_progress('Data/new_movie_comparisons.csv', 'Data/comparison_results.csv')\n",
    "if Movies is None:\n",
    "   Movies = pd.DataFrame(Data)\n",
    "if not Comparison_Results:\n",
    "   Comparison_Results = []\n",
    "Movies, Comparison_Results = evaluate_movies_in_batches(Movies, Batch_Size=Batch_Size, Comparison_Results=Comparison_Results)\n",
    "save_progress(Movies, Comparison_Results, 'Data/new_movie_comparisons.csv', 'Data/comparison_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison Between Predicted Re-Watch Desire and Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Random Forest Predictions\n",
    "Predictions_Data = pd.read_csv('Data/RF_Predictions.csv')\n",
    "\n",
    "# Load the New Movie Comparisons Dataset\n",
    "New_Movie_Comparisons = pd.read_csv('Data/new_movie_comparisons.csv')\n",
    "New_Movie_Comparisons.rename(columns={'Re-Watch Desire': 'Actual Re-Watch Desire'}, inplace=True)\n",
    "\n",
    "# Merge the New Movie Comparisons Dataset with the RF Predictions\n",
    "Merged_Data = pd.merge(Predictions_Data, New_Movie_Comparisons, on='Movie')\n",
    "\n",
    "# Round Both Predicted and Actual Values to one decimal place\n",
    "Merged_Data['Actual Re-Watch Desire (Rounded)'] = Merged_Data['Actual Re-Watch Desire'].round(1)\n",
    "Merged_Data['Predicted Re-Watch Desire (Rounded)'] = Merged_Data['Predicted Re-Watch Desire'].round(1)\n",
    "\n",
    "# Calculate the Difference and round it to one decimal place\n",
    "Merged_Data['Difference'] = (Merged_Data['Predicted Re-Watch Desire (Rounded)'] - \n",
    "                             Merged_Data['Actual Re-Watch Desire (Rounded)']).round(1)\n",
    "\n",
    "# Adjust the Comparison Columns in a Pandas Table Format\n",
    "Results_Comparison = Merged_Data[['Movie', 'Actual Re-Watch Desire (Rounded)', 'Predicted Re-Watch Desire (Rounded)', 'Difference']]\n",
    "\n",
    "# Display the formatted table\n",
    "Results_Comparison.head()\n",
    "\n",
    "# Save the Comparison Result to a CSV File\n",
    "Results_Comparison.to_csv('RF_Results_Comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/RF_Comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overall, the model performs reasonably well for most movies, with minimal differences in predictions for DiDi, Iron Man, and American Beauty. <br><br> The model struggles slightly with movies like Forest Gump and American Psycho, where it underestimates or overestimates the re-watch desire by about one point. <br><br> This indicates that the model may benefit from exposure to more movies like these to improve its prediction accuracy, especially for certain genres or types of films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore What Keeps Me Coming Back to my Favorite Films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Trained Random Forest Model From the Saved File\n",
    "Best_RF_Model = joblib.load('Models/best_rf_model.pkl')\n",
    "\n",
    "# Load Merged_Data\n",
    "Merged_Data = pd.read_csv('Data/Merged_Data.csv')\n",
    "\n",
    "# Extract only the Relevant_Features from the Dataset\n",
    "Relevant_Features = [\n",
    "    'IMDb Rating', 'Box Office (In Millions)', 'MPAA Numeric', 'Award Winner', 'Foreign Film', 'Runtime (min)',\n",
    "    'Budget (In Millions)', 'Action', 'Biography', 'Comedy', 'Crime', 'Drama', 'Fantasy', 'History', 'Music',\n",
    "    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Sport', 'Thriller', 'War'\n",
    "]\n",
    "\n",
    "Relevant_Data = Merged_Data[Relevant_Features]\n",
    "\n",
    "# Define Importance and Feature Names\n",
    "Importances = Best_RF_Model.feature_importances_\n",
    "Feature_Names = Relevant_Data.columns\n",
    "\n",
    "# Sort Features by Importance\n",
    "Indices = np.argsort(Importances)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Feature Importance in Re-Watch Predictions')\n",
    "plt.barh(range(len(Indices)), Importances[Indices], color='#000080', align='center')\n",
    "plt.yticks(range(len(Indices)), [Feature_Names[i] for i in Indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/Feature_Importance_in_Re-Watch_Predictions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "* **IMDb Rating is the most significant factor in predicting Re-Watch Desire, suggesting high ratings align with high Re-Watch Desire**\n",
    "* **Runtime plays a significant role, suggesting a potential interest in watching longer movies rather shorter movies**\n",
    "* **Box Office Earnings suggests a stronger Re-Watch Desire for movies that performed well in theaters and had mass appeal**\n",
    "* **The Comedy genre is a significant feature, suggesting that humor is a key driver of Re-Watch Desire**\n",
    "* **Award Wins and Budget show moderate importance, suggesting critically acclaimed high-budget films are more likely to be re-watched**\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Final_Movies_Data\n",
    "Final_Movies_Data = pd.read_csv('Data/Final_Movies_Data.csv')\n",
    "\n",
    "# Define genre columns\n",
    "genre_columns = ['Action', 'Biography', 'Comedy', 'Crime', 'Drama', 'Fantasy', 'History', \n",
    "                 'Music', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Sport', 'Thriller', 'War']\n",
    "\n",
    "# Calculate the count of movies in each genre\n",
    "genre_counts = Final_Movies_Data[genre_columns].sum()\n",
    "\n",
    "# We melt the data to transform it into a format where we can plot genre vs. re-watch desire\n",
    "melted_data = Final_Movies_Data.melt(id_vars=['Re-Watch Desire'], value_vars=genre_columns, \n",
    "                                     var_name='Genre', value_name='Is_Genre')\n",
    "\n",
    "# Filter only for rows where the genre is relevant (Is_Genre == 1)\n",
    "filtered_data = melted_data[melted_data['Is_Genre'] == 1]\n",
    "\n",
    "# Ensure genres in 'filtered_data' are sorted in the same order as genre_counts\n",
    "filtered_data['Genre'] = pd.Categorical(filtered_data['Genre'], categories=genre_counts.index, ordered=True)\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Genre', y='Re-Watch Desire', data=filtered_data, color='#000080', order=genre_counts.index)\n",
    "\n",
    "# Add the number of movies per genre to the x-axis labels\n",
    "new_labels = [f'{genre} ({int(genre_counts[genre])})' for genre in genre_counts.index]\n",
    "plt.xticks(range(len(new_labels)), new_labels, rotation=45)\n",
    "\n",
    "# Customizing the plot\n",
    "plt.title('Re-Watch Desire Distribution by Genre', fontsize=14)\n",
    "plt.xlabel('Genre', fontsize=12)\n",
    "plt.ylabel('Re-Watch Desire', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/Re-Watch_Desire_Distribution_by_Genre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: One Movie Can Fit into Multiple Genres**\n",
    "\n",
    "* **Romance, Crime, and Drama genres exhibit higher medians, suggesting these genres are more likely to be re-watched**\n",
    "* **Mystery has the highest median, but there are limited movies fitting this genre type in my collection**\n",
    "* **History and Biography films have lower medians, suggesting these genres are less likely to be re-watched**\n",
    "* **Biography, Fantasy, and Sci-Fi films have wide ranges in Re-Watch Desire, indicating these genres evoke a wide range of preferences**\n",
    "* **Mystery and Sport genres have the least variability, but there are also less movies fitting these genre types**\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "Final_Movies_Data = pd.read_csv('Data/Final_Movies_Data.csv')\n",
    "\n",
    "# Define Pie Chart Colors\n",
    "Blue_Shades = ['#73C2FB', '#C6E6FB', '#007FFF', '#CCCCFF']\n",
    "\n",
    "# Group by 'MPAA Numeric' and calculate the mean of 'Re-Watch Desire' for each MPAA Numeric category\n",
    "MPAA_AVG_Re_Watch = Final_Movies_Data.groupby('MPAA Numeric')['Re-Watch Desire'].mean()\n",
    "\n",
    "# Replace the MPAA Numeric values with their corresponding labels (G, PG, PG-13, R)\n",
    "MPAA_Labels = {1: 'G', 2: 'PG', 3: 'PG-13', 4: 'R'}\n",
    "MPAA_AVG_Re_Watch_Labeled = MPAA_AVG_Re_Watch.rename(index=MPAA_Labels)\n",
    "\n",
    "# Calculate how many movies fit into each MPAA category, using the same order as MPAA_AVG_Re_Watch\n",
    "MPAA_Movie_Count = Final_Movies_Data['MPAA Numeric'].map(MPAA_Labels).value_counts().reindex(MPAA_AVG_Re_Watch_Labeled.index)\n",
    "\n",
    "# Create the pie chart with numeric values for the average Re-Watch Desire\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(MPAA_AVG_Re_Watch_Labeled, \n",
    "        labels=[f'{label} ({count})' for label, count in MPAA_Movie_Count.items()],\n",
    "        autopct=lambda p: f'{p * MPAA_AVG_Re_Watch_Labeled.sum() / 100:.2f}', \n",
    "        colors=Blue_Shades)\n",
    "\n",
    "plt.title('Average Re-Watch Desire by MPAA Rating', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/Average_Re-Watch_Desire_by_MPAA_Rating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **R-Rated films have highest Re-Watch Desire, suggesting more mature content tends to be re-watched**\n",
    "* **G-Rated films have second highest Re-Watch Desire, indicating family-friendly or universally accessible films are still enjoyed**\n",
    "* **PG-13 movies have a moderate Re-Watch Desire, possibly explained by these movies catering to audiences of a wider variety**\n",
    "* **PG movies have the lowest Re-Watch Desire, while intended for a broad audience, these films might be less engaging**\n",
    "* **General Implications: this data shows that more mature content tends to draw higher Re-Watch Desire**\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/Final_Movie_Re_Watch.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
